{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd \n",
    "application_df = pd.read_csv(\"Resources/charity_data.csv\")\n",
    "application_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set Up Variables\n",
    "new_df = application_df.drop(['EIN','NAME'], axis = 1)\n",
    "unique = new_df.nunique()\n",
    "selected_cols = [col for col in list(dict(unique).keys()) if dict(unique)[col] > 10][:2]\n",
    "application_counts = new_df[selected_cols[0]].value_counts()\n",
    "classification_counts = new_df[selected_cols[1]].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Bin Application Column and Classifications Column\n",
    "# -------------------------------------------------\n",
    "# Application Set Up\n",
    "# ------------------\n",
    "cut_off = 156\n",
    "application_types_to_replace = [type for type in list(dict(application_counts).keys()) if dict(application_counts)[type] <= cut_off ]\n",
    "\n",
    "for app in application_types_to_replace:\n",
    "    new_df['APPLICATION_TYPE'] = new_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "\n",
    "# Classifications Set Up\n",
    "# ----------------------\n",
    "classifications_to_replace = []\n",
    "min_cut_off = 1\n",
    "cut_off = 1883\n",
    "\n",
    "for type in [x for x in list(dict(classification_counts).keys())]:\n",
    "    if dict(classification_counts)[type] < cut_off or dict(classification_counts)[type] == min_cut_off:\n",
    "        classifications_to_replace.append(type)\n",
    "\n",
    "for cls in classifications_to_replace:\n",
    "    new_df['CLASSIFICATION'] = new_df['CLASSIFICATION'].replace(cls,\"Other\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set up final Dataframe that will used for model training and evaluation\n",
    "# -----------------------------------------------------------------------\n",
    "dum_df = pd.get_dummies(new_df)\n",
    "target = dum_df['IS_SUCCESSFUL']\n",
    "features = dum_df.drop(['IS_SUCCESSFUL'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,target)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Convert to Numpy Arrays to avoid data type issue with TensorFlow\n",
    "scale_train_x = np.asarray(X_train_scaled)\n",
    "train_y = np.asarray(y_train)\n",
    "scale_test_x = np.asarray(X_test_scaled)\n",
    "test_y = np.asarray(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define and fir the deep neural net model \n",
    "# ---------------------------------\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "hidden_nodes_layer1 =  8\n",
    "hidden_nodes_layer2 = 5\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=5, activation=\"relu\", input_dim=number_input_features))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=5, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_model.summary()\n",
    "\n",
    "# fit the model \n",
    "fit_model = nn_model.fit(scale_train_x, train_y, epochs=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate the Model and Save to h5 file\n",
    "# --------------------------------------\n",
    "# Evaluate\n",
    "model_loss, model_accuracy = nn_model.evaluate(scale_test_x ,test_y,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "# Save and export to h5 file\n",
    "nn_model.save(\"AlphabetSoupCharity_Optimization.h5\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}