{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "application_df = pd.read_csv(\"Resources/charity_data.csv\")\n",
    "application_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        EIN                                      NAME APPLICATION_TYPE  \\\n",
       "0  10520599              BLUE KNIGHTS MOTORCYCLE CLUB              T10   \n",
       "1  10531628    AMERICAN CHESAPEAKE CLUB CHARITABLE TR               T3   \n",
       "2  10547893        ST CLOUD PROFESSIONAL FIREFIGHTERS               T5   \n",
       "3  10553066            SOUTHSIDE ATHLETIC ASSOCIATION               T3   \n",
       "4  10556103  GENETIC RESEARCH INSTITUTE OF THE DESERT               T3   \n",
       "\n",
       "        AFFILIATION CLASSIFICATION      USE_CASE  ORGANIZATION  STATUS  \\\n",
       "0       Independent          C1000    ProductDev   Association       1   \n",
       "1       Independent          C2000  Preservation  Co-operative       1   \n",
       "2  CompanySponsored          C3000    ProductDev   Association       1   \n",
       "3  CompanySponsored          C2000  Preservation         Trust       1   \n",
       "4       Independent          C1000     Heathcare         Trust       1   \n",
       "\n",
       "      INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0              0                      N     5000              1  \n",
       "1         1-9999                      N   108590              1  \n",
       "2              0                      N     5000              0  \n",
       "3    10000-24999                      N     6692              1  \n",
       "4  100000-499999                      N   142590              1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10531628</td>\n",
       "      <td>AMERICAN CHESAPEAKE CLUB CHARITABLE TR</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10547893</td>\n",
       "      <td>ST CLOUD PROFESSIONAL FIREFIGHTERS</td>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10553066</td>\n",
       "      <td>SOUTHSIDE ATHLETIC ASSOCIATION</td>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556103</td>\n",
       "      <td>GENETIC RESEARCH INSTITUTE OF THE DESERT</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Set Up Variables\n",
    "# ---------------\n",
    "new_df = application_df.drop(['EIN','NAME'], axis = 1)\n",
    "unique = new_df.nunique()\n",
    "selected_cols = [col for col in list(dict(unique).keys()) if dict(unique)[col] > 10][:2]\n",
    "application_counts = new_df[selected_cols[0]].value_counts()\n",
    "classification_counts = new_df[selected_cols[1]].value_counts()\n",
    "\n",
    "# Bin Application Column and Classifications Column\n",
    "# -------------------------------------------------\n",
    "# Application Set Up\n",
    "# ------------------\n",
    "cut_off = 27037\n",
    "application_types_to_replace = [type for type in list(dict(application_counts).keys()) if dict(application_counts)[type] == cut_off ]\n",
    "\n",
    "for app in application_types_to_replace:\n",
    "    new_df['APPLICATION_TYPE'] = new_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "# Classifications Set Up\n",
    "# ----------------------\n",
    "classifications_to_replace = []\n",
    "min_cut_off = 1\n",
    "cut_off = 1883\n",
    "\n",
    "for type in [x for x in list(dict(classification_counts).keys())]:\n",
    "    if dict(classification_counts)[type] < cut_off or dict(classification_counts)[type] == min_cut_off:\n",
    "        classifications_to_replace.append(type)\n",
    "\n",
    "for cls in classifications_to_replace:\n",
    "    new_df['CLASSIFICATION'] = new_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "\n",
    "new_df['APPLICATION_TYPE'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Other    27037\n",
       "T4        1542\n",
       "T6        1216\n",
       "T5        1173\n",
       "T19       1065\n",
       "T8         737\n",
       "T7         725\n",
       "T10        528\n",
       "T9         156\n",
       "T13         66\n",
       "T12         27\n",
       "T2          16\n",
       "T25          3\n",
       "T14          3\n",
       "T15          2\n",
       "T29          2\n",
       "T17          1\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Added PCA Attempt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Set up final Dataframe that will used for model training and evaluation\n",
    "# -----------------------------------------------------------------------\n",
    "dum_df = pd.get_dummies(new_df)\n",
    "target = dum_df['IS_SUCCESSFUL']\n",
    "#features = dum_df.drop(['IS_SUCCESSFUL','USE_CASE_Other', 'USE_CASE_Preservation', 'USE_CASE_ProductDev','SPECIAL_CONSIDERATIONS_N', 'SPECIAL_CONSIDERATIONS_Y'], axis = 1)\n",
    "features = dum_df.drop(['IS_SUCCESSFUL'], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,target)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Convert to Numpy Arrays to avoid data type issue with TensorFlow\n",
    "scale_train_x = np.asarray(X_train_scaled)\n",
    "train_y = np.asarray(y_train)\n",
    "scale_test_x = np.asarray(X_test_scaled)\n",
    "test_y = np.asarray(y_test)\n",
    "\n",
    "# PCA attempt to reduce number of dimensions \n",
    "pca = PCA(n_components = 0.80)\n",
    "pca_data_train = pca.fit_transform(scale_train_x)\n",
    "pca_data_test = pca.transform(scale_test_x)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define and fit the deep neural net model \n",
    "# ---------------------------------\n",
    "#number_input_features = len(pca_data_train.iloc[0])\n",
    "number_input_features = pca_data_train.shape[1]\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=32, activation=\"relu\", input_dim=number_input_features))\n",
    "\n",
    "# First hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=5, activation=\"tanh\"))\n",
    "\n",
    "# Output layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_model.summary()\n",
    "\n",
    "# Compile the Model \n",
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# fit the model \n",
    "# fit_model = nn_model.fit(scale_train_x, train_y, epochs=50)\n",
    "fit_model = nn_model.fit(pca_data_train, train_y, epochs=5)\n",
    "\n",
    "# Evaluate the Model and Save to h5 file\n",
    "# --------------------------------------\n",
    "model_loss, model_accuracy = nn_model.evaluate(pca_data_test,test_y,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "# Save and export to h5 file\n",
    "nn_model.save(\"AlphabetSoupCharity_Optimization.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Without PCA Attempy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Set up final Dataframe that will used for model training and evaluation\n",
    "# -----------------------------------------------------------------------\n",
    "dum_df = pd.get_dummies(new_df)\n",
    "target = dum_df['IS_SUCCESSFUL']\n",
    "#features = dum_df.drop(['IS_SUCCESSFUL','USE_CASE_Other', 'USE_CASE_Preservation', 'USE_CASE_ProductDev','SPECIAL_CONSIDERATIONS_N', 'SPECIAL_CONSIDERATIONS_Y'], axis = 1)\n",
    "features = dum_df.drop(['IS_SUCCESSFUL'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,target)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Convert to Numpy Arrays to avoid data type issue with TensorFlow\n",
    "scale_train_x = np.asarray(X_train_scaled)\n",
    "train_y = np.asarray(y_train)\n",
    "scale_test_x = np.asarray(X_test_scaled)\n",
    "test_y = np.asarray(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Define and fit the deep neural net model \n",
    "# ---------------------------------\n",
    "#number_input_features = len(pca_data_train.iloc[0])\n",
    "number_input_features = len(scale_train_x[0])\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=32, activation=\"relu\", input_dim=number_input_features))\n",
    "\n",
    "# First hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=5, activation=\"tanh\"))\n",
    "\n",
    "# Output layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_model.summary()\n",
    "\n",
    "# Compile the Model \n",
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# fit the model \n",
    "# fit_model = nn_model.fit(scale_train_x, train_y, epochs=50)\n",
    "fit_model = nn_model.fit(scale_train_x, train_y, epochs=25)\n",
    "\n",
    "# Evaluate the Model and Save to h5 file\n",
    "# --------------------------------------\n",
    "model_loss, model_accuracy = nn_model.evaluate(scale_test_x,test_y,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "# Save and export to h5 file\n",
    "nn_model.save(\"AlphabetSoupCharity_Optimization.h5\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_53 (Dense)             (None, 32)                1664      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 5)                 85        \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 2,555\n",
      "Trainable params: 2,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25724 samples\n",
      "Epoch 1/25\n",
      "25724/25724 [==============================] - 7s 257us/sample - loss: 0.5742 - accuracy: 0.7147\n",
      "Epoch 2/25\n",
      "25724/25724 [==============================] - 5s 176us/sample - loss: 0.5553 - accuracy: 0.7285\n",
      "Epoch 3/25\n",
      "25724/25724 [==============================] - 4s 138us/sample - loss: 0.5523 - accuracy: 0.7309\n",
      "Epoch 4/25\n",
      "25724/25724 [==============================] - 4s 160us/sample - loss: 0.5512 - accuracy: 0.7305\n",
      "Epoch 5/25\n",
      "25724/25724 [==============================] - 4s 144us/sample - loss: 0.5493 - accuracy: 0.7341\n",
      "Epoch 6/25\n",
      "25724/25724 [==============================] - 3s 128us/sample - loss: 0.5484 - accuracy: 0.7329\n",
      "Epoch 7/25\n",
      "25724/25724 [==============================] - 3s 131us/sample - loss: 0.5477 - accuracy: 0.7327\n",
      "Epoch 8/25\n",
      "25724/25724 [==============================] - 4s 144us/sample - loss: 0.5476 - accuracy: 0.7328\n",
      "Epoch 9/25\n",
      "25724/25724 [==============================] - 4s 170us/sample - loss: 0.5465 - accuracy: 0.7324\n",
      "Epoch 10/25\n",
      "25724/25724 [==============================] - 4s 136us/sample - loss: 0.5457 - accuracy: 0.7337\n",
      "Epoch 11/25\n",
      "25724/25724 [==============================] - 3s 129us/sample - loss: 0.5459 - accuracy: 0.7335\n",
      "Epoch 12/25\n",
      "25724/25724 [==============================] - 4s 137us/sample - loss: 0.5451 - accuracy: 0.7341\n",
      "Epoch 13/25\n",
      "25724/25724 [==============================] - 5s 189us/sample - loss: 0.5448 - accuracy: 0.7338\n",
      "Epoch 14/25\n",
      "25724/25724 [==============================] - 4s 166us/sample - loss: 0.5448 - accuracy: 0.7336\n",
      "Epoch 15/25\n",
      "25724/25724 [==============================] - 5s 192us/sample - loss: 0.5442 - accuracy: 0.7332\n",
      "Epoch 16/25\n",
      "25724/25724 [==============================] - 4s 151us/sample - loss: 0.5440 - accuracy: 0.7354\n",
      "Epoch 17/25\n",
      "25724/25724 [==============================] - 4s 145us/sample - loss: 0.5441 - accuracy: 0.7338\n",
      "Epoch 18/25\n",
      "25724/25724 [==============================] - 5s 184us/sample - loss: 0.5439 - accuracy: 0.7348\n",
      "Epoch 19/25\n",
      "25724/25724 [==============================] - 4s 169us/sample - loss: 0.5436 - accuracy: 0.7349\n",
      "Epoch 20/25\n",
      "25724/25724 [==============================] - 4s 162us/sample - loss: 0.5436 - accuracy: 0.7352\n",
      "Epoch 21/25\n",
      "25724/25724 [==============================] - 4s 175us/sample - loss: 0.5435 - accuracy: 0.7343\n",
      "Epoch 22/25\n",
      "25724/25724 [==============================] - 5s 186us/sample - loss: 0.5432 - accuracy: 0.7348\n",
      "Epoch 23/25\n",
      "25724/25724 [==============================] - 4s 164us/sample - loss: 0.5432 - accuracy: 0.7360\n",
      "Epoch 24/25\n",
      "25724/25724 [==============================] - 5s 177us/sample - loss: 0.5430 - accuracy: 0.7345\n",
      "Epoch 25/25\n",
      "25724/25724 [==============================] - 5s 187us/sample - loss: 0.5425 - accuracy: 0.7360\n",
      "8575/1 - 2s - loss: 0.5572 - accuracy: 0.7304\n",
      "Loss: 0.5529952692568476, Accuracy: 0.7303789854049683\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Tuner Attempt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import keras_tuner as kt\n",
    "import random\n",
    "\n",
    "number_input_features = len(pca_data_train[0])\n",
    "\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','tanh',\"softmax\"])\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=6,\n",
    "        max_value=32,\n",
    "        step=2), activation=activation, input_dim=number_input_features))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=6,\n",
    "            max_value=32,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model\n",
    "directory_name  = \"Tuner\" + str(random.randint(1,1234456))\n",
    "tuner = kt.Hyperband(create_model,objective=\"val_accuracy\",max_epochs=5,hyperband_iterations=2,directory = directory_name)\n",
    "tuner.search(pca_data_train,train_y,epochs=5,validation_data=(pca_data_test,test_y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trial 20 Complete [00h 00m 41s]\n",
      "val_accuracy: 0.7329446077346802\n",
      "\n",
      "Best val_accuracy So Far: 0.7351603507995605\n",
      "Total elapsed time: 00h 09m 50s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Attempted PCA to lower number \n",
    "# Unbinning the application type data column actually increaseased the accuracy of the NN\n",
    "# tuner attempt achieved best accuracy with pca components = .90, lowering pca ratio slightly increased accuracy"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('venv': conda)"
  },
  "interpreter": {
   "hash": "3f6acd4545d008d74b1bc56e0920897854c77ecf9e874c3b3700a6e370b33431"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}