{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "application_df = pd.read_csv(\"Resources/charity_data.csv\")\n",
    "application_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        EIN                                      NAME APPLICATION_TYPE  \\\n",
       "0  10520599              BLUE KNIGHTS MOTORCYCLE CLUB              T10   \n",
       "1  10531628    AMERICAN CHESAPEAKE CLUB CHARITABLE TR               T3   \n",
       "2  10547893        ST CLOUD PROFESSIONAL FIREFIGHTERS               T5   \n",
       "3  10553066            SOUTHSIDE ATHLETIC ASSOCIATION               T3   \n",
       "4  10556103  GENETIC RESEARCH INSTITUTE OF THE DESERT               T3   \n",
       "\n",
       "        AFFILIATION CLASSIFICATION      USE_CASE  ORGANIZATION  STATUS  \\\n",
       "0       Independent          C1000    ProductDev   Association       1   \n",
       "1       Independent          C2000  Preservation  Co-operative       1   \n",
       "2  CompanySponsored          C3000    ProductDev   Association       1   \n",
       "3  CompanySponsored          C2000  Preservation         Trust       1   \n",
       "4       Independent          C1000     Heathcare         Trust       1   \n",
       "\n",
       "      INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0              0                      N     5000              1  \n",
       "1         1-9999                      N   108590              1  \n",
       "2              0                      N     5000              0  \n",
       "3    10000-24999                      N     6692              1  \n",
       "4  100000-499999                      N   142590              1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10531628</td>\n",
       "      <td>AMERICAN CHESAPEAKE CLUB CHARITABLE TR</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10547893</td>\n",
       "      <td>ST CLOUD PROFESSIONAL FIREFIGHTERS</td>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10553066</td>\n",
       "      <td>SOUTHSIDE ATHLETIC ASSOCIATION</td>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556103</td>\n",
       "      <td>GENETIC RESEARCH INSTITUTE OF THE DESERT</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Set Up Variables\n",
    "# ---------------\n",
    "# new_df = application_df.drop(['EIN','NAME','ASK_AMT'], axis = 1)\n",
    "new_df = application_df.drop(['EIN','NAME'], axis = 1)\n",
    "unique = new_df.nunique()\n",
    "selected_cols = [col for col in list(dict(unique).keys()) if dict(unique)[col] > 10][:2]\n",
    "application_counts = new_df[selected_cols[0]].value_counts()\n",
    "classification_counts = new_df[selected_cols[1]].value_counts()\n",
    "\n",
    "# Bin Application Column and Classifications Column\n",
    "# -------------------------------------------------\n",
    "# Application Set Up\n",
    "# ------------------\n",
    "cut_off = 27037\n",
    "application_types_to_replace = [type for type in list(dict(application_counts).keys()) if dict(application_counts)[type] == cut_off ]\n",
    "\n",
    "for app in application_types_to_replace:\n",
    "    new_df['APPLICATION_TYPE'] = new_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "# Classifications Set Up\n",
    "# ----------------------\n",
    "classifications_to_replace = []\n",
    "min_cut_off = 1\n",
    "cut_off = 1883\n",
    "\n",
    "for type in [x for x in list(dict(classification_counts).keys())]:\n",
    "    if dict(classification_counts)[type] < cut_off or dict(classification_counts)[type] == min_cut_off:\n",
    "        classifications_to_replace.append(type)\n",
    "\n",
    "for cls in classifications_to_replace:\n",
    "    new_df['CLASSIFICATION'] = new_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "\n",
    "new_df['APPLICATION_TYPE'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Other    27037\n",
       "T4        1542\n",
       "T6        1216\n",
       "T5        1173\n",
       "T19       1065\n",
       "T8         737\n",
       "T7         725\n",
       "T10        528\n",
       "T9         156\n",
       "T13         66\n",
       "T12         27\n",
       "T2          16\n",
       "T14          3\n",
       "T25          3\n",
       "T15          2\n",
       "T29          2\n",
       "T17          1\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Added PCA Attempt (TO BE USED WITH TUNER)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Set up final Dataframe that will used for model training and evaluation\n",
    "# -----------------------------------------------------------------------\n",
    "dum_df = pd.get_dummies(new_df)\n",
    "target = dum_df['IS_SUCCESSFUL']\n",
    "#features = dum_df.drop(['IS_SUCCESSFUL','USE_CASE_Other', 'USE_CASE_Preservation', 'USE_CASE_ProductDev','SPECIAL_CONSIDERATIONS_N', 'SPECIAL_CONSIDERATIONS_Y'], axis = 1)\n",
    "features = dum_df.drop(['IS_SUCCESSFUL'], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,target)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Convert to Numpy Arrays to avoid data type issue with TensorFlow\n",
    "scale_train_x = np.asarray(X_train_scaled)\n",
    "train_y = np.asarray(y_train)\n",
    "scale_test_x = np.asarray(X_test_scaled)\n",
    "test_y = np.asarray(y_test)\n",
    "\n",
    "# PCA attempt to reduce number of dimensions \n",
    "pca = PCA(n_components = 0.80)\n",
    "pca_data_train = pca.fit_transform(scale_train_x)\n",
    "pca_data_test = pca.transform(scale_test_x)\n",
    "\n",
    "pca_data_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(25724, 29)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Without PCA Attempy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Set up final Dataframe that will used for model training and evaluation\n",
    "# -----------------------------------------------------------------------\n",
    "dum_df = pd.get_dummies(new_df)\n",
    "target = dum_df['IS_SUCCESSFUL']\n",
    "#features = dum_df.drop(['IS_SUCCESSFUL','USE_CASE_Other', 'USE_CASE_Preservation', 'USE_CASE_ProductDev','SPECIAL_CONSIDERATIONS_N', 'SPECIAL_CONSIDERATIONS_Y'], axis = 1)\n",
    "features = dum_df.drop(['IS_SUCCESSFUL'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,target)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Convert to Numpy Arrays to avoid data type issue with TensorFlow\n",
    "scale_train_x = np.asarray(X_train_scaled)\n",
    "train_y = np.asarray(y_train)\n",
    "scale_test_x = np.asarray(X_test_scaled)\n",
    "test_y = np.asarray(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Define and fit the deep neural net model \n",
    "# ---------------------------------\n",
    "#number_input_features = len(pca_data_train.iloc[0])\n",
    "number_input_features = len(scale_train_x[0])\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=512, activation=\"relu\", input_dim=number_input_features))\n",
    "\n",
    "# First hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=32, activation=\"tanh\"))\n",
    "\n",
    "# Output layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_model.summary()\n",
    "\n",
    "# Compile the Model \n",
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# fit the model \n",
    "# fit_model = nn_model.fit(scale_train_x, train_y, epochs=50)\n",
    "fit_model = nn_model.fit(scale_train_x, train_y, epochs=25)\n",
    "\n",
    "# Evaluate the Model and Save to h5 file\n",
    "# --------------------------------------\n",
    "model_loss, model_accuracy = nn_model.evaluate(scale_test_x,test_y,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "# Save and export to h5 file\n",
    "nn_model.save(\"AlphabetSoupCharity_Optimization.h5\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               26112     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 194,497\n",
      "Trainable params: 194,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25724 samples\n",
      "Epoch 1/25\n",
      "25724/25724 [==============================] - 11s 439us/sample - loss: 0.5650 - accuracy: 0.7225\n",
      "Epoch 2/25\n",
      "25724/25724 [==============================] - 9s 333us/sample - loss: 0.5557 - accuracy: 0.7282\n",
      "Epoch 3/25\n",
      "25724/25724 [==============================] - 10s 384us/sample - loss: 0.5521 - accuracy: 0.7306\n",
      "Epoch 4/25\n",
      "25724/25724 [==============================] - 9s 337us/sample - loss: 0.5501 - accuracy: 0.7293\n",
      "Epoch 5/25\n",
      "25724/25724 [==============================] - 9s 359us/sample - loss: 0.5488 - accuracy: 0.7306\n",
      "Epoch 6/25\n",
      "25724/25724 [==============================] - 9s 343us/sample - loss: 0.5476 - accuracy: 0.7315\n",
      "Epoch 7/25\n",
      "25724/25724 [==============================] - 10s 374us/sample - loss: 0.5480 - accuracy: 0.7331\n",
      "Epoch 8/25\n",
      "25724/25724 [==============================] - 9s 367us/sample - loss: 0.5471 - accuracy: 0.7309\n",
      "Epoch 9/25\n",
      "25724/25724 [==============================] - 9s 340us/sample - loss: 0.5459 - accuracy: 0.7320\n",
      "Epoch 10/25\n",
      "25724/25724 [==============================] - 9s 335us/sample - loss: 0.5461 - accuracy: 0.7320\n",
      "Epoch 11/25\n",
      "25724/25724 [==============================] - 11s 422us/sample - loss: 0.5475 - accuracy: 0.7319\n",
      "Epoch 12/25\n",
      "25724/25724 [==============================] - 9s 356us/sample - loss: 0.5474 - accuracy: 0.7322\n",
      "Epoch 13/25\n",
      "25724/25724 [==============================] - 9s 347us/sample - loss: 0.5474 - accuracy: 0.7304\n",
      "Epoch 14/25\n",
      "25724/25724 [==============================] - 10s 404us/sample - loss: 0.5455 - accuracy: 0.7346\n",
      "Epoch 15/25\n",
      "25724/25724 [==============================] - 9s 359us/sample - loss: 0.5456 - accuracy: 0.7326\n",
      "Epoch 16/25\n",
      "25724/25724 [==============================] - 9s 364us/sample - loss: 0.5441 - accuracy: 0.7343\n",
      "Epoch 17/25\n",
      "25724/25724 [==============================] - 11s 426us/sample - loss: 0.5450 - accuracy: 0.7334\n",
      "Epoch 18/25\n",
      "25724/25724 [==============================] - 11s 429us/sample - loss: 0.5444 - accuracy: 0.7322\n",
      "Epoch 19/25\n",
      "25724/25724 [==============================] - 11s 447us/sample - loss: 0.5461 - accuracy: 0.7315\n",
      "Epoch 20/25\n",
      "25724/25724 [==============================] - 9s 359us/sample - loss: 0.5462 - accuracy: 0.7321\n",
      "Epoch 21/25\n",
      "25724/25724 [==============================] - 12s 455us/sample - loss: 0.5450 - accuracy: 0.7329\n",
      "Epoch 22/25\n",
      "25724/25724 [==============================] - 16s 612us/sample - loss: 0.5462 - accuracy: 0.7349\n",
      "Epoch 23/25\n",
      "25724/25724 [==============================] - 9s 361us/sample - loss: 0.5463 - accuracy: 0.7338\n",
      "Epoch 24/25\n",
      "25724/25724 [==============================] - 11s 434us/sample - loss: 0.5450 - accuracy: 0.7326\n",
      "Epoch 25/25\n",
      "25724/25724 [==============================] - 11s 415us/sample - loss: 0.5444 - accuracy: 0.7332\n",
      "8575/1 - 1s - loss: 0.5100 - accuracy: 0.7280\n",
      "Loss: 0.5592211006125626, Accuracy: 0.7280466556549072\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Tuner Attempt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import keras_tuner as kt\n",
    "import random\n",
    "\n",
    "number_input_features = len(pca_data_train[0])\n",
    "\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','tanh',\"softmax\"])\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=6,\n",
    "        max_value=32,\n",
    "        step=2), activation=activation, input_dim=number_input_features))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=6,\n",
    "            max_value=32,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model\n",
    "directory_name  = \"Tuner\" + str(random.randint(1,1234456))\n",
    "tuner = kt.Hyperband(create_model,objective=\"val_accuracy\",max_epochs=5,hyperband_iterations=2,directory = directory_name)\n",
    "tuner.search(pca_data_train,train_y,epochs=5,validation_data=(pca_data_test,test_y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trial 20 Complete [00h 00m 41s]\n",
      "val_accuracy: 0.7329446077346802\n",
      "\n",
      "Best val_accuracy So Far: 0.7351603507995605\n",
      "Total elapsed time: 00h 09m 50s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Get second best model hyperparameters\n",
    "hypers = tuner.get_best_hyperparameters(2)[0]\n",
    "hypers.values"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'activation': 'softmax',\n",
       " 'first_units': 30,\n",
       " 'num_layers': 2,\n",
       " 'units_0': 20,\n",
       " 'units_1': 24,\n",
       " 'units_2': 20,\n",
       " 'tuner/epochs': 5,\n",
       " 'tuner/initial_epoch': 0,\n",
       " 'tuner/bracket': 0,\n",
       " 'tuner/round': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Attempted PCA to lower number \n",
    "# Unbinning the application type data column actually increaseased the accuracy of the NN\n",
    "# tuner attempt achieved best accuracy with pca components = .90, lowering pca ratio slightly increased accuracy"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('venv': conda)"
  },
  "interpreter": {
   "hash": "3f6acd4545d008d74b1bc56e0920897854c77ecf9e874c3b3700a6e370b33431"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}